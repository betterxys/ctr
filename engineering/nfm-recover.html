<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <div class="toc">
   <ul>
    <li>
     <a href="#nfm-afm">
      NFM / AFM 复现
     </a>
     <ul>
      <li>
       <a href="#_1">
        数据集
       </a>
      </li>
      <li>
       <a href="#_2">
        模型
       </a>
       <ul>
        <li>
         <a href="#nfm">
          NFM
         </a>
        </li>
        <li>
         <a href="#afm">
          AFM
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a href="#_3">
        实验
       </a>
       <ul>
        <li>
         <a href="#nfm_1">
          NFM 实验
         </a>
        </li>
        <li>
         <a href="#afm_1">
          AFM 模型
         </a>
        </li>
        <li>
         <a href="#afm-vs-nfm-vs-fm">
          AFM vs NFM vs FM
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </div>
  <h2 id="nfm-afm">
   NFM / AFM 复现
  </h2>
  <h3 id="_1">
   数据集
  </h3>
  <p>
   Frappe
  </p>
  <ul>
   <li>
    <p>
     train/validation/test: 202,027 / 57,722 / 28, 860
    </p>
   </li>
   <li>
    <p>
     feature: 10(离散特征) -&gt; 所有可取特征值共计 5382
    </p>
   </li>
  </ul>
  <h3 id="_2">
   模型
  </h3>
  <p>
   本文复现模型包括： NFM， AFM， 并采用 FM 与之进行对比；
  </p>
  <h4 id="nfm">
   NFM
  </h4>
  <p>
   NFM 主要结构：
  </p>
  <ul>
   <li>
    <p>
     embedding layer: 将每个 feature 转换为一个 k 维 embedding;
    </p>
   </li>
   <li>
    <p>
     Bi-interaction layer: 求解二阶交叉特征， 输出一个 k 维向量;
    </p>
   </li>
   <li>
    <p>
     deep layer: 将 BI 层的 k 维输出，输入到 MLP 中，旨在求解高阶交叉项；
    </p>
   </li>
   <li>
    <p>
     output layer： 全局偏置量 + 原始特征加权求和 + MLP的输出；
    </p>
   </li>
  </ul>
  <h4 id="afm">
   AFM
  </h4>
  <p>
   AFM 的主要结构：
  </p>
  <ul>
   <li>
    <p>
     embedding layer: 将每个 feature 转换为一个 k 维 embedding;
    </p>
   </li>
   <li>
    <p>
     Pair-wise Interaction Layer: 两两 element product, 输出 n(n-1)/2 个 k 维向量；
    </p>
   </li>
   <li>
    <p>
     Attention Based Pooling: 将每个 interaction 的 k 维向量作为 attention net 的输入， attention net由一层 hidden layer 和一层拥有 k 个节点的 output layer 构成， attention net 的输出是 k 维向量， 代表着 input interaction 的 k 个元素分别对应的权重， 将 attention net 的输出与 interaction进行 element product, 并求和，作为输出；
    </p>
   </li>
   <li>
    <p>
     prediction layer: 对全局偏置量、线性加权和、attention based pooling 求和；
    </p>
   </li>
  </ul>
  <p>
   chenglong 对 attention based pooling 进行改进：
  </p>
  <p>
   原文 attention net 的输入是单个 feature interaction 的 k 维 embedding， 所以 attention 起到的作用是对每个 feature interaction 进行一个 pooling 操作， 其输出影响的是每个 feature interaction 内部 k 个维度在进行 sum pooling 时的权重；
  </p>
  <p>
   而 chenglong 的做法是先对每个 feature interaction 做 pooling 操作， 得到 n(n-1)/2个 scale, 然后将这 n(n-1)/2 个 scale 输入 attention net , 此时, attention net 的输出其实是这 n(n-1)/2 个scale feature interaction 进行 pooling 操作时的权重， 这样更符合原文希望剔除无用 interactions 的干扰的愿景；
  </p>
  <h3 id="_3">
   实验
  </h3>
  <h4 id="nfm_1">
   NFM 实验
  </h4>
  <ul>
   <li>
    <em>
     hidden_factor
    </em>
   </li>
  </ul>
  <table>
   <thead>
    <tr>
     <th>
      epoch
     </th>
     <th>
      batch_size
     </th>
     <th>
      hidden_factor
     </th>
     <th>
      keep
     </th>
     <th>
      lr
     </th>
     <th>
      optimizer
     </th>
     <th>
      params
     </th>
     <th>
      train
     </th>
     <th>
      validation
     </th>
     <th>
      test
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      20
     </td>
     <td>
      128
     </td>
     <td>
      64
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      354055
     </td>
     <td>
      0.1601
     </td>
     <td>
      0.338
     </td>
     <td>
      0.3448
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      128
     </td>
     <td>
      64
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      354055
     </td>
     <td>
      0.1163
     </td>
     <td>
      0.322
     </td>
     <td>
      0.3286
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      128
     </td>
     <td>
      64
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      354055
     </td>
     <td>
      0.1023
     </td>
     <td>
      0.3167
     </td>
     <td>
      0.3233
     </td>
    </tr>
    <tr>
     <td>
      20
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1461
     </td>
     <td>
      0.3299
     </td>
     <td>
      0.3316
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0858
     </td>
     <td>
      0.3136
     </td>
     <td>
      0.3176
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0723
     </td>
     <td>
      0.3086
     </td>
     <td>
      0.3120
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   本实验对比了 NFM 在不同的 hidden_factor 下的运行结果， 结果显示 hidden_facor 越大， 收敛越快， 运行结果越好， 但同时意味着模型参数变得更多， 模型更复杂；
  </p>
  <ul>
   <li>
    <em>
     batch_size
    </em>
   </li>
  </ul>
  <table>
   <thead>
    <tr>
     <th>
      epoch
     </th>
     <th>
      batch_size
     </th>
     <th>
      hidden_factor
     </th>
     <th>
      keep
     </th>
     <th>
      lr
     </th>
     <th>
      optimizer
     </th>
     <th>
      params
     </th>
     <th>
      train
     </th>
     <th>
      validation
     </th>
     <th>
      test
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      20
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1461
     </td>
     <td>
      0.3299
     </td>
     <td>
      0.3316
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0858
     </td>
     <td>
      0.3136
     </td>
     <td>
      0.3176
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0723
     </td>
     <td>
      0.3086
     </td>
     <td>
      0.3120
     </td>
    </tr>
    <tr>
     <td>
      20
     </td>
     <td>
      4096
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.2077
     </td>
     <td>
      0.3613
     </td>
     <td>
      0.3625
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      4096
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1344
     </td>
     <td>
      0.3284
     </td>
     <td>
      0.3312
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      4096
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1060
     </td>
     <td>
      0.3191
     </td>
     <td>
      0.3209
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   本实验验证 batch_size 对 NFM 运行结果的影响，  可见更小的 batch_size, 收敛更快， 且效果更好；
  </p>
  <ul>
   <li>
    <em>
     learning rate
    </em>
   </li>
  </ul>
  <table>
   <thead>
    <tr>
     <th>
      epoch
     </th>
     <th>
      batch_size
     </th>
     <th>
      hidden_factor
     </th>
     <th>
      keep
     </th>
     <th>
      lr
     </th>
     <th>
      optimizer
     </th>
     <th>
      params
     </th>
     <th>
      train
     </th>
     <th>
      validation
     </th>
     <th>
      test
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      20
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1461
     </td>
     <td>
      0.3299
     </td>
     <td>
      0.3316
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0858
     </td>
     <td>
      0.3136
     </td>
     <td>
      0.3176
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0723
     </td>
     <td>
      0.3086
     </td>
     <td>
      0.3120
     </td>
    </tr>
    <tr>
     <td>
      20
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.01
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1555
     </td>
     <td>
      0.3366
     </td>
     <td>
      0.3412
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.01
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1115
     </td>
     <td>
      0.3202
     </td>
     <td>
      0.324
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.01
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0964
     </td>
     <td>
      0.3143
     </td>
     <td>
      0.3178
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   更小的学习率会有更慢的收敛速度
  </p>
  <ul>
   <li>
    <em>
     optimizer
    </em>
   </li>
  </ul>
  <table>
   <thead>
    <tr>
     <th>
      epoch
     </th>
     <th>
      batch_size
     </th>
     <th>
      hidden_factor
     </th>
     <th>
      keep
     </th>
     <th>
      lr
     </th>
     <th>
      optimizer
     </th>
     <th>
      params
     </th>
     <th>
      train
     </th>
     <th>
      validation
     </th>
     <th>
      test
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      20
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.1461
     </td>
     <td>
      0.3299
     </td>
     <td>
      0.3316
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0858
     </td>
     <td>
      0.3136
     </td>
     <td>
      0.3176
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adagrad
     </td>
     <td>
      702599
     </td>
     <td>
      0.0723
     </td>
     <td>
      0.3086
     </td>
     <td>
      0.3120
     </td>
    </tr>
    <tr>
     <td>
      20
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adam
     </td>
     <td>
      702599
     </td>
     <td>
      0.4581
     </td>
     <td>
      0.5092
     </td>
     <td>
      0.5126
     </td>
    </tr>
    <tr>
     <td>
      50
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adam
     </td>
     <td>
      702599
     </td>
     <td>
      0.3853
     </td>
     <td>
      0.4586
     </td>
     <td>
      0.4604
     </td>
    </tr>
    <tr>
     <td>
      100
     </td>
     <td>
      128
     </td>
     <td>
      128
     </td>
     <td>
      [0.8,0.5]
     </td>
     <td>
      0.05
     </td>
     <td>
      adam
     </td>
     <td>
      702599
     </td>
     <td>
      0.4383
     </td>
     <td>
      0.4998
     </td>
     <td>
      0.4983
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   adam 的表现极差
  </p>
  <h4 id="afm_1">
   AFM 模型
  </h4>
  <ul>
   <li>
    <em>
     batch_size
    </em>
   </li>
  </ul>
  <table>
   <thead>
    <tr>
     <th>
      [hidden nodes, embedding]
     </th>
     <th>
      attention
     </th>
     <th>
      batch
     </th>
     <th>
      lr
     </th>
     <th>
      optimizer
     </th>
     <th>
      params
     </th>
     <th>
      #epoch
     </th>
     <th>
      train
     </th>
     <th>
      valid
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      4096
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      20
     </td>
     <td>
      0.2092
     </td>
     <td>
      0.3748
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      4096
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      50
     </td>
     <td>
      0.149
     </td>
     <td>
      0.3548
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      4096
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      100
     </td>
     <td>
      0.1213
     </td>
     <td>
      0.3466
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      20
     </td>
     <td>
      0.1536
     </td>
     <td>
      0.3541
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      50
     </td>
     <td>
      0.1243
     </td>
     <td>
      0.3453
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      100
     </td>
     <td>
      0.1111
     </td>
     <td>
      0.3415
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   更小的 batch_size 可以更快的收敛
  </p>
  <ul>
   <li>
    <em>
     hidden_layer
    </em>
   </li>
  </ul>
  <table>
   <thead>
    <tr>
     <th>
      [hidden nodes, embedding]
     </th>
     <th>
      attention
     </th>
     <th>
      batch
     </th>
     <th>
      lr
     </th>
     <th>
      optimizer
     </th>
     <th>
      params
     </th>
     <th>
      #epoch
     </th>
     <th>
      train
     </th>
     <th>
      valid
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      20
     </td>
     <td>
      0.1536
     </td>
     <td>
      0.3541
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      50
     </td>
     <td>
      0.1243
     </td>
     <td>
      0.3453
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      100
     </td>
     <td>
      0.1111
     </td>
     <td>
      0.3415
     </td>
    </tr>
    <tr>
     <td>
      [128,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      711047
     </td>
     <td>
      20
     </td>
     <td>
      0.1527
     </td>
     <td>
      0.3541
     </td>
    </tr>
    <tr>
     <td>
      [128,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      711047
     </td>
     <td>
      50
     </td>
     <td>
      0.1238
     </td>
     <td>
      0.3452
     </td>
    </tr>
    <tr>
     <td>
      [128,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      711047
     </td>
     <td>
      100
     </td>
     <td>
      0.1113
     </td>
     <td>
      0.3416
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   hidden  layer 的节点数目对结果并无太大影响
  </p>
  <ul>
   <li>
    <em>
     embedding
    </em>
   </li>
  </ul>
  <table>
   <thead>
    <tr>
     <th>
      [hidden nodes, embedding]
     </th>
     <th>
      attention
     </th>
     <th>
      batch
     </th>
     <th>
      lr
     </th>
     <th>
      optimizer
     </th>
     <th>
      params
     </th>
     <th>
      #epoch
     </th>
     <th>
      train
     </th>
     <th>
      valid
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      20
     </td>
     <td>
      0.1536
     </td>
     <td>
      0.3541
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      50
     </td>
     <td>
      0.1243
     </td>
     <td>
      0.3453
     </td>
    </tr>
    <tr>
     <td>
      [16,128]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      696487
     </td>
     <td>
      100
     </td>
     <td>
      0.1111
     </td>
     <td>
      0.3415
     </td>
    </tr>
    <tr>
     <td>
      [16,64]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      350951
     </td>
     <td>
      20
     </td>
     <td>
      0.2145
     </td>
     <td>
      0.3743
     </td>
    </tr>
    <tr>
     <td>
      [16,64]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      350951
     </td>
     <td>
      50
     </td>
     <td>
      0.1829
     </td>
     <td>
      0.3649
     </td>
    </tr>
    <tr>
     <td>
      [16,64]
     </td>
     <td>
      1
     </td>
     <td>
      128
     </td>
     <td>
      0.05
     </td>
     <td>
      Adagrad
     </td>
     <td>
      350951
     </td>
     <td>
      100
     </td>
     <td>
      0.1691
     </td>
     <td>
      0.3619
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   embedding 越大， 收敛越快， 效果越好
  </p>
  <h4 id="afm-vs-nfm-vs-fm">
   AFM vs NFM vs FM
  </h4>
  <p>
   设置 hidden layer = 64, embedding = 128, batch = 128, lr = 0.05, optimizer = adagrad 对比 NFM 和 AFM 模型效果；
  </p>
  <table>
   <thead>
    <tr>
     <th>
      model
     </th>
     <th>
      attention
     </th>
     <th>
      param
     </th>
     <th>
      epoch
     </th>
     <th>
      train
     </th>
     <th>
      valid
     </th>
    </tr>
   </thead>
   <tbody>
    <tr>
     <td>
      NFM
     </td>
     <td>
      -
     </td>
     <td>
      702599
     </td>
     <td>
      20
     </td>
     <td>
      0.1461
     </td>
     <td>
      0.3299
     </td>
    </tr>
    <tr>
     <td>
      NFM
     </td>
     <td>
      -
     </td>
     <td>
      702599
     </td>
     <td>
      50
     </td>
     <td>
      0.0858
     </td>
     <td>
      0.3136
     </td>
    </tr>
    <tr>
     <td>
      NFM
     </td>
     <td>
      -
     </td>
     <td>
      702599
     </td>
     <td>
      100
     </td>
     <td>
      0.0723
     </td>
     <td>
      0.3086
     </td>
    </tr>
    <tr>
     <td>
      AFM
     </td>
     <td>
      1
     </td>
     <td>
      702727
     </td>
     <td>
      20
     </td>
     <td>
      0.1518
     </td>
     <td>
      0.3551
     </td>
    </tr>
    <tr>
     <td>
      AFM
     </td>
     <td>
      1
     </td>
     <td>
      702727
     </td>
     <td>
      50
     </td>
     <td>
      0.1237
     </td>
     <td>
      0.3446
     </td>
    </tr>
    <tr>
     <td>
      AFM
     </td>
     <td>
      1
     </td>
     <td>
      702727
     </td>
     <td>
      100
     </td>
     <td>
      0.1114
     </td>
     <td>
      0.3423
     </td>
    </tr>
    <tr>
     <td>
      AFM
     </td>
     <td>
      2
     </td>
     <td>
      700231
     </td>
     <td>
      20
     </td>
     <td>
      0.0857
     </td>
     <td>
      0.3324
     </td>
    </tr>
    <tr>
     <td>
      AFM
     </td>
     <td>
      2
     </td>
     <td>
      700231
     </td>
     <td>
      50
     </td>
     <td>
      0.0562
     </td>
     <td>
      0.3531
     </td>
    </tr>
    <tr>
     <td>
      AFM
     </td>
     <td>
      2
     </td>
     <td>
      700231
     </td>
     <td>
      100
     </td>
     <td>
      0.0294
     </td>
     <td>
      0.3436
     </td>
    </tr>
    <tr>
     <td>
      FM
     </td>
     <td>
      -
     </td>
     <td>
      694279
     </td>
     <td>
      20
     </td>
     <td>
      0.4484
     </td>
     <td>
      0.4965
     </td>
    </tr>
    <tr>
     <td>
      FM
     </td>
     <td>
      -
     </td>
     <td>
      694279
     </td>
     <td>
      50
     </td>
     <td>
      0.3641
     </td>
     <td>
      0.4482
     </td>
    </tr>
    <tr>
     <td>
      FM
     </td>
     <td>
      -
     </td>
     <td>
      694279
     </td>
     <td>
      100
     </td>
     <td>
      0.3078
     </td>
     <td>
      0.4226
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   结论： 在数据集 frappe 上， 相同参数设置下， NFM 的效果要强于 AFM ， NFM 和 AFM 都要强过 FM； 而 chenglong 改进过的 AFM 明显在训练集上可以更快的收敛， 但却存在过拟合的问题；
  </p>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>