<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <h1 id="ctr">
   CTR综述
  </h1>
  <div class="toc">
   <ul>
    <li>
     <a href="#ctr">
      CTR综述
     </a>
     <ul>
      <li>
       <a href="#ctr_1">
        CTR的应用场景
       </a>
      </li>
      <li>
       <a href="#ctr_2">
        CTR预测需要解决的问题
       </a>
      </li>
      <li>
       <a href="#ctr_3">
        CTR的主流技术
       </a>
       <ul>
        <li>
         <a href="#fm-factorization-machine">
          FM (Factorization machine)
         </a>
        </li>
        <li>
         <a href="#ffm-field-aware-factorization-machine">
          FFM (Field-aware Factorization Machine)
         </a>
        </li>
        <li>
         <a href="#wide-deep-model">
          Wide &amp; Deep model
         </a>
        </li>
        <li>
         <a href="#fnnfactorization-machine-supported-neural-network">
          FNN(Factorization-machine supported Neural Network)
         </a>
        </li>
        <li>
         <a href="#snnsampling-based-neural-network">
          SNN(Sampling-based Neural Network)
         </a>
        </li>
        <li>
         <a href="#pnnproduct-based-neural-network">
          PNN(Product-based Neural Network)
         </a>
        </li>
        <li>
         <a href="#deepfmdeep-factorization-machine">
          DeepFM(Deep Factorization Machine)
         </a>
        </li>
        <li>
         <a href="#neural-factorization-machines2017">
          Neural Factorization Machines(2017)
         </a>
        </li>
        <li>
         <a href="#attention-factorization-machines2017">
          Attention Factorization Machines(2017)
         </a>
        </li>
        <li>
         <a href="#deepcross">
          DeepCross
         </a>
        </li>
        <li>
         <a href="#others">
          others
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a href="#ct">
        CT预测总体流程
       </a>
      </li>
      <li>
       <a href="#_1">
        参考文献
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </div>
  <hr/>
  <h2 id="ctr_1">
   CTR的应用场景
  </h2>
  <p>
   CTR主要应用在广告系统和推荐系统当中，其目的在于预估出用户点击相应广告/推荐物品的概率。
  </p>
  <p>
   CTR在推荐系统中的应用方式主要有两种：
  </p>
  <ol>
   <li>
    <p>
     推荐列表依据 \( CTR \) 的值进行排序
    </p>
   </li>
   <li>
    <p>
     推荐列表依据 \( CTR*bid \) 的值进行排序（ \(bid\)是指如果用户点击该推荐，商家可以获得的收益）
    </p>
   </li>
  </ol>
  <p>
   CTR在广告系统中的应用也是类似，平台会根据广告收益和CTR的值对广告进行排序。
  </p>
  <h2 id="ctr_2">
   CTR预测需要解决的问题
  </h2>
  <ol>
   <li>
    <p>
     难以发掘有效的
     <strong>
      交叉特征
     </strong>
     ， 尤其是高阶交叉特征；
    </p>
   </li>
   <li>
    <p>
     传统 ML 方法难以适用于超
     <strong>
      高维
     </strong>
     度的
     <strong>
      稀疏
     </strong>
     数据；
    </p>
   </li>
  </ol>
  <h2 id="ctr_3">
   CTR的主流技术
  </h2>
  <h3 id="fm-factorization-machine">
   FM (Factorization machine)
  </h3>
  <p>
   FM 的出现主要想解决的问题是 SVM 在高维稀疏空间难以训练的问题， 现已成为稀疏数据下预测问题的经典算法。 FM 包括三部分：全局偏置量、 原始特征加权求和项、 二阶交叉项， 其中二阶交叉是 FM 的关键之处，每个特征对应有一个隐向量， 通过对所有特征的隐向量两两做内积， 在求和组成二阶交叉项。FM的模型可以表示为：
  </p>
  <p>
   <mathjax>
    $$
\hat{y}(x) := w_0 + 
\sum_{i=1}^n w_i x_i +
\sum_{i = 1}^{n} \sum_{j = i + 1}^{n} &lt;V_i, V_j&gt; x_{i} x_{j}
$$
   </mathjax>
  </p>
  <p>
   <a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">
    原文链接
   </a>
   <a href="https://github.com/ibayer/fastFM">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/fm.html">
    阅读记录
   </a>
  </p>
  <h3 id="ffm-field-aware-factorization-machine">
   FFM (Field-aware Factorization Machine)
  </h3>
  <p>
   FFM 是获得了 criteo 和 avazu 的冠军模型，FFM 本质是 FM 模型的一个变体， 对 FM 进行改进， FM 当中每个特征只有一个隐变量， 是与其余所有变量共享的， 而 FFM 模型通过引入 field 的概念， 每个特征对每个不同的 field 都有不同的隐变量对应， 有更强的灵活性和适应性。
  </p>
  <p>
   <mathjax>
    $$
\phi_{FFM}(w,x) = \sum_{j_1=1}^{n} \sum_{j_2=j_1+1}^n
(w_{j_1, f_2} \cdot w_{j_2, f_1}) x_{j_1}x_{j_2}
$$
   </mathjax>
  </p>
  <p>
   <a href="https://www.andrew.cmu.edu/user/yongzhua/conferences/ffm.pdf">
    原文链接
   </a>
   <a href="https://github.com/guestwalk/libffm">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/fm.html">
    阅读记录
   </a>
  </p>
  <h3 id="wide-deep-model">
   Wide &amp; Deep model
  </h3>
  <p>
   <img _="," alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/wdm.png" width="900"/>
   <br/>
   Wide &amp; Deep 模型 是 2016 年 Google 开源于 TensorFlow 的一种混合网络结构 , Wide &amp; Deep model 的出现是因为作者认为， 交叉特征的生成主要通过记忆式和生成式两种方式， 记忆式就是数据当中出现过的， 能够体现出来的特征交叉， 而生成式是指现有数据当中没有体现出来的隐式的交叉特征， 记忆式的特征提取较为简单， Wide &amp; Deep 当中的 Wide 就是用于提取这部分交叉特征的， 而隐式的生成式特征交叉就只能通过深度学习深入挖掘， 所以 deep 部分的主要职能就是为了挖掘这部分生成式交叉特征。
  </p>
  <p>
   <a href="https://arxiv.org/pdf/1606.07792.pdf">
    原文链接
   </a>
   <a href="https://www.tensorflow.org/tutorials/wide_and_deep">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/wdm.html">
    阅读记录
   </a>
  </p>
  <h3 id="fnnfactorization-machine-supported-neural-network">
   FNN(Factorization-machine supported Neural Network)
  </h3>
  <p>
   <img alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/FNN.png"/>
  </p>
  <p>
   FNN 拟解决的关键问题是高维稀疏特征难以通过神经网络进行训练，FNN 借鉴了 FM 的思路解决高维稀疏问题。FNN 在全连接的 hidden layer 之前添加了一个 Dense Real Layer 将原始的高维稀疏向量转化为低维连续向量， 训练时，预训练 FM , 然后以 FM 训练的 embedding 初始化 Dense Real Layer 的参数, 此外作者采用了 partial 连接的方式， 减少了参数的数量并建议将这种方法推广到更高层 。
  </p>
  <h3 id="snnsampling-based-neural-network">
   SNN(Sampling-based Neural Network)
  </h3>
  <p>
   <img alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/snn_all.png"/>
  </p>
  <p>
   SNN与FNN发表于同一篇论文， 二者的基本结构相同，主要的区别有两点：
  </p>
  <ul>
   <li>
    <p>
     SNN的第一层是全连接的，而FNN的第一层是基于Field进行b部分连接的;
    </p>
   </li>
   <li>
    <p>
     SNN的第一层权重初始化采用 Sampling-based RBM/DAE， 而FNN的第一层权重初始化需要进行FM的预训练;
    </p>
   </li>
  </ul>
  <p>
   此外，本文 SNN 所谓 Sampling based 是指，选中 one hot 的向量中非零的一项， 再随机选中 m 个值为零的项参与训练，如此一来可以减小训练复杂度;
  </p>
  <p>
   <a href="https://arxiv.org/pdf/1601.02376.pdf">
    原文链接
   </a>
   <a href="https://github.com/wnzhang/deep-ctr">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/fnn.html">
    阅读记录
   </a>
  </p>
  <h3 id="pnnproduct-based-neural-network">
   PNN(Product-based Neural Network)
  </h3>
  <p>
   <img alt="" height="350" src="https://betterxys.github.io/styles/images/ctrfigs/PNN.png"/>
   <br/>
   PNN的结构很清晰，针对CTR预测问题的两个难点： 特征向量高维稀疏和交叉特征难以提取，提出了解决方案：
  </p>
  <ul>
   <li>
    <p>
     借鉴 FM 应对高维稀疏的方法， 在 input layer 之后添加一层 embedding layer， 将高维稀疏向量转化为低维连续向量解决高维稀疏带来的问题；
    </p>
   </li>
   <li>
    <p>
     在 embedding layer 之后添加 product layer， product layer 由 z 和 p 两部分组成， z 是原始特征和常数1做 product, 其实就是为了保留原始特征， 而 p 部分可以有选择的采用 inner product 或者 outter product 对 embedding 向量两两 product, 两部分同时作为 hidden layer 的输入， 通过 hidden layer 深入挖掘高阶交叉特征；
    </p>
   </li>
  </ul>
  <p>
   此外， 值得注意的是 PNN 采用了下采样的操作解决数据分布不均的问题。
  </p>
  <p>
   <a href="https://arxiv.org/pdf/1611.00144.pdf">
    原文链接
   </a>
   <a href="https://github.com/Atomu2014/product-nets">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/pnn.html">
    阅读记录
   </a>
  </p>
  <h3 id="deepfmdeep-factorization-machine">
   DeepFM(Deep Factorization Machine)
  </h3>
  <p>
   <img alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/deepfm.png"/>
   <br/>
   DeepFM 结合了 FM 和 DNN 模型，该模型主要分为 deep 和 wide 两部分，deep 模块使用 deep learning 学习高阶交叉特征，wide 模块使用 FM 提取低阶交叉特征，deep和wide两个模块共享输入和embeddings，最终通过output layer进行结合，输出CTR。
  </p>
  <p>
   DeepFM 和 wide &amp; deep 的区别在于:
  </p>
  <ul>
   <li>
    DeepFM 的 wide 部分由 FM 组成， 不需要人工参与特征构造；
   </li>
   <li>
    DeepFM 的 input 和 embedding 由 deep 和 wide 两部分共享；
   </li>
  </ul>
  <p>
   <a href="https://arxiv.org/pdf/1703.04247.pdf">
    原文链接
   </a>
   <a href="https://github.com/Leavingseason/OpenLearning4DeepRecsys">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/deepfm.html">
    阅读记录
   </a>
  </p>
  <h3 id="neural-factorization-machines2017">
   Neural Factorization Machines(2017)
  </h3>
  <p>
   <img alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/NFM.png"/>
  </p>
  <p>
   Neural FM 其实是 FM 的一个神经网络的实现版， 可以轻松复现 FM， 同时， 由于采用神经网络的结构实现了 FM， 所以可以在 FM 之后添加 hidden layer 来深化 FM 以学习非线性的高阶交叉特征。 Neural FM 的关键在于其 Bi-Interaction layer, 如果去掉后续的 hidden layer 它可以复现 FM 的效果， 而令 Neural FM 与其他神经网络不同的地方同样在于 Bi-Interaction layer, 其他的网络结构， 在输入数据进网络时， 一般都是简单的加权平均， 而 NFM 则是做了两两内积求和生成一个向量的 pooling 操作， 大大简化了后续 hidden layer 的训练复杂度。
  </p>
  <p>
   <a href="https://arxiv.org/pdf/1708.05027.pdf">
    原文链接
   </a>
   <a href="https://github.com/hexiangnan/neural factorization machine">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/nfm.html">
    阅读记录
   </a>
  </p>
  <h3 id="attention-factorization-machines2017">
   Attention Factorization Machines(2017)
  </h3>
  <p>
   <img alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/AFM.png"/>
  </p>
  <p>
   AFM 和 NFM 来自于同一人，AFM 模型主要解决的问题在于， FM 模型在学习交叉特征的时候， 对所有的交叉特征项赋予了相同的权重， 而在实际情况当中， 不可能所有的特征对结果都有相同的影响， 所以， AFM 在原有 FM 的基础上， 添加 Attention 机制， 旨在赋予不同的交叉特征以不同的权重。
  </p>
  <p>
   <a href="https://arxiv.org/pdf/1708.04617.pdf">
    原文链接
   </a>
   <a href="https://github.com/hexiangnan/attentional factorization machine">
    参考代码
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/afm.html">
    阅读记录
   </a>
  </p>
  <h3 id="deepcross">
   DeepCross
  </h3>
  <p>
   <img alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/deepcross.png"/>
  </p>
  <p>
   deepcross 模型的结果如上图所示，主要包括四层： embedding layer, stacking layer, residual units 和 scoring layer。embedding layer 将高维稀疏向量转换为低维连续向量， stacking layer 将所有 feature 的 embedding 向量 concat 到一起输入到 residual layers 学习交叉特征， 最终到 scoring layer 输出预测结果。 该模型最主要的贡献在于引入了 residual units 并取得了较好的效果。
  </p>
  <p>
   <a href="http://www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf">
    原文链接
   </a>
   <a href="http://htmlpreview.github.io/?https://github.com/betterxys/ctr/blob/master/survey/deepcross.html">
    阅读记录
   </a>
  </p>
  <h3 id="others">
   others
  </h3>
  <ul>
   <li>
    <p>
     FTRL(Follow the Regularied Leader)
     <br/>
     FTRL是一种基于逻辑回归的
     <strong>
      在线学习算法
     </strong>
     ,能够学习出有效且稀疏的模型， FTRL 采用的依然是线性模型， 更倾向于工程方案。
    </p>
   </li>
   <li>
    <p>
     GBDT + LR
     <br/>
     Facebook的论文尝试提出一种
     <a href="https://github.com/neal668/LightGBM-GBDT-LR">
      解决特征组合问题的方案
     </a>
     ,基本思路是利用树模型的组合特性来自动做特征组合, 结合 GBDT 训练出一些组合特征, 将 GBDT 叶节点的输出结果传入 LR 进行分类,该方法取得极大成功,但也有很大程度的过拟合风险,所以必须要采取相应的防止过拟合的措施。
    </p>
   </li>
   <li>
    <p>
     RNN
     <br/>
     更倾向于序列数据
    </p>
   </li>
   <li>
    <p>
     CNN
     <br/>
     更倾向于由相邻特征之间的相互作用而产生的交叉特征.
    </p>
   </li>
  </ul>
  <h2 id="ct">
   CT预测总体流程
  </h2>
  <p>
   通常， CTR预测包括如下流程：
  </p>
  <ul>
   <li>
    <p>
     特征预处理
    </p>
    <ul>
     <li>
      连续特征标准化
     </li>
     <li>
      连续特征离散化(离散特征更易于提取交叉特征)
     </li>
     <li>
      离散特征二值化
     </li>
    </ul>
   </li>
   <li>
    <p>
     embedding layer
    </p>
    <p>
     embedding layer 的作用是将高维稀疏特征转换为低维连续特征， 虽然大体的思路类似，但这里是否可以考虑引入 FFM 的 Field 的概念， 每个 feature 对应多个隐向量。
    </p>
   </li>
   <li>
    <p>
     feature interaction
    </p>
    <p>
     仅仅是原始特征很难达到好的效果，所以需要挖掘交叉特征，当前交叉特征的学习方式只有两类： product 和 hidden layer。
    </p>
    <ul>
     <li>
      <p>
       PNN 的作者尝试了 inner product 和 outter product， 但两者并没有表现出过大的差异， 所以 product 的方式基本可以锁定为 inner product， 可以考虑改进的方式是与 embedding layer 结合采取 FFM 的 field 的思路， 最好是改进为一个可以复现 FFM 的 NN 模型；
      </p>
     </li>
     <li>
      <p>
       hidden layer 可以操作的空间较大， DeepCross 的 residual unit 可以应用到 hidden layer, 甚至 AFM 的 attention 机制也可以应用到 hidden layer 的输入层， 此外 dropout / bn / early stopping 应该都只算是该模型优化的一些小技巧；
      </p>
     </li>
    </ul>
   </li>
   <li>
    <p>
     output layer
    </p>
    <p>
     output layer一般都是sigmoid function
    </p>
   </li>
  </ul>
  <h2 id="_1">
   参考文献
  </h2>
  <div class="footnote">
   <hr/>
   <ol>
    <li id="fn:1">
     <p>
      Rendle, Steffen. “Factorization machines with libfm.” ACM Transactions on Intelligent Systems and Technology (TIST) 3.3 (2012): 57.
      <a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:2">
     <p>
      CHENG H-T, KOC L, HARMSEN J, et al. Wide &amp; deep learning for recommender sys-tems[C] // Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. 2016 : 7 – 10.
      <a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:3">
     <p>
      Jahrer, Michael, et al. “Ensemble of collaborative filtering and feature engineered models for click through rate prediction.” KDDCup Workshop. 2012.
      <a class="footnote-backref" href="#fnref:3" rev="footnote" title="Jump back to footnote 3 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:4">
     <p>
      MCMAHAN H B, STREETER M. Adaptive bound optimization for online convex opti-mization[J]. arXiv preprint arXiv:1002.4908, 2010.
      <a class="footnote-backref" href="#fnref:4" rev="footnote" title="Jump back to footnote 4 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:5">
     <p>
      MCMAHAN H B, HOLT G, SCULLEY D, et al. Ad click prediction: a view from thetrenches[C] // Proceedings of the 19th ACM SIGKDD international conference on Knowl-edge discovery and data mining. 2013 : 1222 – 1230.
      <a class="footnote-backref" href="#fnref:5" rev="footnote" title="Jump back to footnote 5 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:6">
     <p>
      HE X, PAN J, JIN O, et al. Practical lessons from predicting clicks on ads at facebook[C]// Proceedings of the Eighth International Workshop on Data Mining for Online Advertising.2014 : 1 – 9.
      <a class="footnote-backref" href="#fnref:6" rev="footnote" title="Jump back to footnote 6 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:7">
     <p>
      Zhang, Yuyu, et al. “Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks.” AAAI. 2014.
      <a class="footnote-backref" href="#fnref:7" rev="footnote" title="Jump back to footnote 7 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:8">
     <p>
      Liu, Qiang, et al. “A convolutional click prediction model.” Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015.
      <a class="footnote-backref" href="#fnref:8" rev="footnote" title="Jump back to footnote 8 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:9">
     <p>
      ZHANG W, DU T, WANG J. Deep learning over multi-field categorical data[C] // Europeanconference on information retrieval. 2016 : 45 – 57.
      <a class="footnote-backref" href="#fnref:9" rev="footnote" title="Jump back to footnote 9 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:10">
     <p>
      QU Y, CAI H, REN K, et al. Product-based neural networks for user response prediction[C]// Data Mining (ICDM), 2016 IEEE 16th International Conference on. 2016 : 1149 – 1154.
      <a class="footnote-backref" href="#fnref:10" rev="footnote" title="Jump back to footnote 10 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:11">
     <p>
      GUO H, TANG R, YE Y, et al. DeepFM: A Factorization-Machine based Neural Networkfor CTR Prediction[J]. arXiv preprint arXiv:1703.04247, 2017.
      <a class="footnote-backref" href="#fnref:11" rev="footnote" title="Jump back to footnote 11 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:12">
     <p>
      HE X, CHUA T-S. Neural Factorization Machines for Sparse Predictive Analytics[J], 2017.
      <a class="footnote-backref" href="#fnref:12" rev="footnote" title="Jump back to footnote 12 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:13">
     <p>
      XIAO J, YE H, HE X, et al. Attentional factorization machines: Learning the weight of feature interactions via attention networks[J]. arXiv preprint arXiv:1708.04617, 2017.
      <a class="footnote-backref" href="#fnref:13" rev="footnote" title="Jump back to footnote 13 in the text">
       ↩
      </a>
     </p>
    </li>
    <li id="fn:14">
     <p>
      Steffen Rendle (2012): Factorization Machines with libFM, in ACM Trans. Intell. Syst. Technol., 3(3), May.
      <a class="footnote-backref" href="#fnref:14" rev="footnote" title="Jump back to footnote 14 in the text">
       ↩
      </a>
     </p>
    </li>
   </ol>
  </div>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>