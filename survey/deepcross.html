<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <h1 id="deep-crossing-web-scale-modeling-without-manualy-crafted-combinatorial-features">
   Deep Crossing: Web-Scale Modeling without Manualy Crafted Combinatorial Features
  </h1>
  <p>
   <a href="http://www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf">
    原文链接
   </a>
  </p>
  <p>
   DeepCross 的目的是为了自动产生交叉特征， 避免大量的手动特征工程， 对密集型和稀 疏型的数据具有同样的效果；
  </p>
  <h2 id="deepcross">
   DeepCross 结构
  </h2>
  <p>
   <img alt="" height="500" src="https://betterxys.github.io/styles/images/ctrfigs/deepcross.png"/>
  </p>
  <p>
   deepcross 模型的结果如上图所示，主要包括四层： embedding layer, stacking layer, residual units 和 scoring layer, 目标函数采用 logloss。
  </p>
  <h3 id="embedding-layer">
   embedding layer
  </h3>
  <p>
   embedding layer 是一个单层的神经网络， 采用 ReLU 作为激活函数：
  </p>
  <p>
   <mathjax>
    $$
x_j^O = max(0, W_j X_j^I + b_j)
$$
   </mathjax>
  </p>
  <p>
   DeepCross 的 embedding 和其他模型的有些区别， 这里的 embedding layer 几乎都是借鉴的 FM， 采用的线性方式， 这里有 ReLU 的激活函数；
  </p>
  <h3 id="stacking-layer">
   stacking layer
  </h3>
  <p>
   <mathjax>
    $$
X^O = [x_0^o, x_1^o, ..., x_n^o]
$$
   </mathjax>
  </p>
  <p>
   stacking layer 是将上层 embedding layer 的输出 concat 为一个 n 维向量， 这里的 n 就是 Feature 的数量。
  </p>
  <p>
   值得注意的是， 本文 embedding 的维度设置为 256 维， 并且有一个细节是，embedding 仅仅针对 one hot 过后维度大于 256 维的特征， 而保留不足256维的特征， 直接参与 stacking layer, 就像图中的 Feature #2;
  </p>
  <h3 id="residual-layer">
   residual layer
  </h3>
  <p>
   <img alt="" height="300" src="https://betterxys.github.io/styles/images/ctrfigs/residualunit.png"/>
  </p>
  <p>
   本文采用了简化版的 residual unit， 如上图所示， 输入向量经过两层 ReLU 单元后的输出， 再加上原始的输入， 组成了 residual unit 的输出值。
  </p>
  <p>
   <mathjax>
    $$
X^O = F(X^I, \{ W_o, W_1 \}, \{ b_0, b_1 \}) + X^I
$$
   </mathjax>
  </p>
  <p>
   从上式可以看出， F 拟合的是输出值与输出值的差， 这也是 residual 的含义；
  </p>
  <h3 id="scoring-layer">
   scoring layer
  </h3>
  <p>
   最终的 scoring layer 是一个 sigmoid 单元
  </p>
  <h2 id="_1">
   实现
  </h2>
  <p>
   本文作者在论文中给出了 CNTK 版的实现：
  </p>
  <pre><code>ONELAYER(Dim_XO, Dim_XI, XI){
W = Parameter(Dim_XO, Dim_XI)
b = Parameter(Dim_XO)
XO = Plus(Times(W, XI), b)
}

ONELAYERSIG(Dim_XO, Dim_XI, XI){
t = ONELAYER(Dim_XO, Dim_XI, XI)
XO = Sigmoid(t)
}

ONELAYERRELU(Dim_XO, Dim_XI, XI){
t = ONELAYER(Dim_XO, Dim_XI, XI)
XO = ReLU(t)
}

RESIDUALUNIT(Dim_H, Dim_XI, XI){
l1 = ONELAYERRELU(Dim_H, Dim_XI, XI)
l2 = ONELAYER(Dim_XI, Dim_H, l1)
XO = ReLU(Plus(XI, l2))
}

## The Deep Crossing Model
## Step 1: Read in features, omitted
## Step 2A: Embedding
Q = ONELAYERRELU(Dim_E, Dim_Query, Query)
K = ONELAYERRELU(Dim_E, Dim_Keyword, Keyword)
T = ONELAYERRELU(Dim_E, Dim_Title, Title)
C = ONELAYERRELU(Dim_E, Dim_CampaignID, CampaignID)
## Step 2B: Stacking
# M = MatchType, CC = CampaignIDCount
Stack = RowStack(Q, K, T, C, M, CC)
## Step 3: Deep Residual Layers
r1 = RESIDUALUNIT(Dim_CROSS1, Dim_Stack, Stack)
r2 = RESIDUALUNIT(Dim_CROSS2, Dim_Stack, r1)
r3 = RESIDUALUNIT(Dim_CROSS3, Dim_Stack, r2)
r4 = RESIDUALUNIT(Dim_CROSS4, Dim_Stack, r3)
r5 = RESIDUALUNIT(Dim_CROSS5, Dim_Stack, r4)
## Step 4: Final Sigmoid Layer
Predict = ONELAYERSIG(Dim_L, Dim_Stack, r5)
## Step 5: Log Loss Objective
CE = LogLoss(Label, Predict)
CriteriaNodes = (CE)
</code></pre>
  <p>
   其中模型参数设置：
  </p>
  <p>
   Dim_E = 256,  # embedding dim
   <br/>
   Dim_CROSS[1-5] = [512, 512, 256, 128, 64]  # residual size
  </p>
  <h2 id="_2">
   后续思考
  </h2>
  <p>
   本文对离散特征的处理， 与其他论文有写区别， 作者仅保留离散特征的最频繁的10000个离散值， 其他的通过统计量来描述 (这块的具体实现方式一直没有看太明白， 详见论文4.1-Individual Featuers) ， 但是最后又通过实验表明， 不加这些统计量的时候模型性能表现， 要明显强于加上统计量， 本人对这块有点模糊， 抛却统计量的事情， 是不是应该对比一下仅保留 top 10000 和全部保留的性能区别？
  </p>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>