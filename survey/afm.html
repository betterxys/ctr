<!DOCTYPE HTML>
<html>
 <head>
  <meta charset="utf-8"/>
  <title>
   Made with Remarkable!
  </title>
  <link href="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/styles/github.min.css" rel="stylesheet"/>
  <style type="text/css">
   body,table tr{background-color:#fff}table tr td,table tr th{border:1px solid #ccc;text-align:left;padding:6px 13px;margin:0}pre code,table,table tr{padding:0}hr,pre code{background:0 0}body{font:16px Helvetica,Arial,sans-serif;line-height:1.4;color:#333;word-wrap:break-word;padding:10px 15px}strong,table tr th{font-weight:700}h1{font-size:2em;margin:.67em 0;text-align:center}h2{font-size:1.75em}h3{font-size:1.5em}h4{font-size:1.25em}h1,h2,h3,h4,h5,h6{font-weight:700;position:relative;margin-top:15px;margin-bottom:15px;line-height:1.1}h1,h2{border-bottom:1px solid #eee}hr{height:0;margin:15px 0;overflow:hidden;border:0;border-bottom:1px solid #ddd}a{color:#4183C4}a.absent{color:#c00}ol,ul{padding-left:15px;margin-left:5px}ol{list-style-type:lower-roman}table tr{border-top:1px solid #ccc;margin:0}table tr:nth-child(2n){background-color:#aaa}table tr td :first-child,table tr th :first-child{margin-top:0}table tr td:last-child,table tr th :last-child{margin-bottom:0}img{max-width:100%}blockquote{padding:0 15px;border-left:4px solid #ccc}code,tt{margin:0 2px;padding:0 5px;white-space:nowrap;border:1px solid #eaeaea;background-color:#f8f8f8;border-radius:3px}pre code{margin:0;white-space:pre;border:none}.highlight pre,pre{background-color:#f8f8f8;border:1px solid #ccc;font-size:13px;line-height:19px;overflow:auto;padding:6px 10px;border-radius:3px}
  </style>
 </head>
 <body>
  <h1 id="attentional-factorization-machines">
   Attentional Factorization Machines
  </h1>
  <p>
   本文提出的 AFM 模型主要解决的问题在于， FM 模型在学习交叉特征的时候， 对所有的交叉特征项赋予了相同的权重， 而在实际情况当中， 不可能所有的特征对结果都有相同的影响， 所以， 本文在原有 FM 的基础上， 添加 Attention 机制， 旨在赋予不同的交叉特征以不同的权重。
  </p>
  <p>
   attention FM 的优势在于：
  </p>
  <ul>
   <li>
    更好的性能表现；
   </li>
   <li>
    更好的可解释性， 可以进一步分析具体的交叉特征对结果的影响；
   </li>
  </ul>
  <h2 id="afm">
   AFM 的结构
  </h2>
  <p>
   AFM 包括三部分， 全局偏置项、 原始特征加权求和项、 交叉特征的学习项（如下图所示）。
  </p>
  <p>
   <mathjax>
    $$
\hat{y}_{AFM}(x) = w_0 + \sum_{i=1}^n w_i x_i +
p^T \sum_{i=1}^n \sum_{j=i+1}^{n} a_{ij} (v_i \bigodot v_j) x_i x_j
$$
   </mathjax>
  </p>
  <p>
   <img alt="" height="500" src="https://betterxys.github.io/styles/images/ctrfigs/AFM.png"/>
  </p>
  <h3 id="embedding-layer">
   Embedding Layer
  </h3>
  <p>
   AFM 的 input layer 和 embedding layer 与 FM / NFM 是一致的， 都是将原有特征转换为 embedding 向量再参与后续计算，
  </p>
  <h3 id="pair-wise-interaction-layer">
   Pair-wise Interaction Layer
  </h3>
  <p>
   Pair-wise Interaction Layer 和 Neural FM 的 Bi-Interaction Layer 其实是一样的；
  </p>
  <p>
   <mathjax>
    $$
f_{PI}(\varepsilon) = {(v_i \bigodot v_j) x_i x_j}
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
R_x = \{ (i, j) \}_{i \in X, j \in X, j&gt;i}
$$
   </mathjax>
  </p>
  <h3 id="attention-based-pooling-layer">
   Attention-based Pooling Layer
  </h3>
  <p>
   <mathjax>
    $$
f_{Att}(f_{PI}(\varepsilon)) = \sum_{(i,j)\in R_x} a_{ij} (v_i \bigodot v_j) x_i x_j
$$
   </mathjax>
  </p>
  <p>
   其中 \( a_{ij} \) 是 特征 i 和特征 j 的交叉特征的 attention score， 本文的 attention net 是单层隐含层，但
   <strong>
    后续可以考虑 Attention 部分采用 MLP 来学习 attention score
   </strong>
   , 对从未出现过的交叉特征进行评分， 目前单层只能学习到数据中出现的交叉特征的权重；
  </p>
  <p>
   本文 attention-based pooling 层的输出是一个 k 维向量， k 是 embedding 向量的维度， 这和 Neural FM 类似， NFM BI层的输出也是 k 维向量；
  </p>
  <p>
   <mathjax>
    $$
a_{ij} = softmax(a'_{ij})
$$
   </mathjax>
  </p>
  <p>
   <mathjax>
    $$
a'_{ij} = h^T ReLU(W(v_i \bigodot v_j) x_i x_j + b)
$$
   </mathjax>
  </p>
  <ul>
   <li>
    attention net 隐含层的节点数称为 attention factor， 对应的是 attention score 的个数;
   </li>
   <li>
    attention net 隐含层的激活函数采用 ReLU;
   </li>
   <li>
    attention score 通过 softmax 进行标准化处理;
   </li>
  </ul>
  <h2 id="_1">
   训练技巧
  </h2>
  <ul>
   <li>
    <p>
     对 Pair-wise Interaction Layer 使用 dropout
    </p>
   </li>
   <li>
    <p>
     attention net 不适用 dropout
    </p>
   </li>
   <li>
    <p>
     目标函数加入 L2 正则化
    </p>
   </li>
  </ul>
  <h2 id="future-research">
   future research
  </h2>
  <ul>
   <li>
    探索能够有效获取高阶交叉特征的方法
   </li>
   <li>
    在 Attention-based Pooling 后添加 MLP
   </li>
   <li>
    consider improving its learning efficiency, for example by using
    <strong>
     learning to hash
    </strong>
    and
    <strong>
     data sampling techniques
    </strong>
    .
   </li>
   <li>
    develop FM variants for semi-supervised and multi-view learning, for example by incorporating the widely used
    <strong>
     graph Laplacian
    </strong>
    and
    <strong>
     co-regularization designs
    </strong>
    .
   </li>
  </ul>
  <h3 id="paper">
   需要继续学习的paper
  </h3>
  <p>
   以下 paper 都是 NFM 和 AFM 当中提到的性能相关的 future work， 优先级不高。
  </p>
  <ul>
   <li>
    <p>
     data sampling
    </p>
    <ul>
     <li>
      Meng Wang, Weijie Fu, Shijie Hao, Hengchang Liu, and Xindong Wu. Learning on big graph: Label inference and regularization with anchor hierarchy. IEEE TKDE, 2017.
     </li>
    </ul>
   </li>
   <li>
    <p>
     co-regularization
    </p>
    <ul>
     <li>
      <p>
       <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.2280&amp;rep=rep1&amp;type=pdf">
        Xiangnan He, Min-Yen Kan, Peichu Xie, and Xiao Chen. Comment-based multi-view clustering of web 2.0 items. In WWW, 2014.
       </a>
      </p>
     </li>
     <li>
      <p>
       Yang Yang, Zhigang Ma, Yi Yang, Feiping Nie, and Heng Tao Shen. Multitask spectral clustering by exploring intertask correlation. IEEE TCYB, 2015.
      </p>
     </li>
    </ul>
   </li>
   <li>
    <p>
     hashing tech
    </p>
    <ul>
     <li>
      <p>
       <a href="http://www.ee.columbia.edu/~wliu/CVPR15_SDH.pdf">
        Fumin Shen, Chunhua Shen, Wei Liu, and Heng Tao Shen. Supervised discrete hashing. In CVPR, 2015.
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="http://www.ee.columbia.edu/~wliu/SIGIR17_binarizing.pdf">
        Shen, Fumin, et al. “Classification by Retrieval: Binarizing Data and Classifier.” (2017).
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="https://pdfs.semanticscholar.org/ef86/1dbbc8779e8d83bc588b23b0bd25fbaa9f36.pdf">
        Zhang, Hanwang, et al. “Discrete collaborative filtering.” Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2016.
       </a>
      </p>
     </li>
    </ul>
   </li>
   <li>
    <p>
     graph Laplacian
    </p>
    <ul>
     <li>
      <p>
       <a href="https://www.comp.nus.edu.sg/~kanmy/papers/sigir2014_he.pdf">
        X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama. Predicting the popularity of web 2.0 items based on user comments. In SIGIR, 2014.
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="http://www.projectsgoal.com/download_projects/data-mining/data-mining-projects-GDM00056.pdf">
        M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. Scalable semi-supervised learning by efficient anchor graph regularization. IEEE Transaction on Knowledge and
        <br/>
        Data Engineering, 2016.
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="http://www.comp.nus.edu.sg/~xiangnan/papers/tkde16-birank-cr.pdf">
        Xiangnan He, Ming Gao, Min-Yen Kan, and Dingxian Wang. BiRank: Towards ranking on bipartite graphs. IEEE TKDE, 2017.
       </a>
      </p>
     </li>
    </ul>
   </li>
  </ul>
  <script src="http://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.1/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad();
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>
  <script type="text/javascript">
   MathJax.Hub.Config({"showProcessingMessages" : false,"messageStyle" : "none","tex2jax": { inlineMath: [ [ "$", "$" ] ] }});
  </script>
 </body>
</html>